# ==============================================================================
# Nimbus - A Document Mind | Configuration Template
# ==============================================================================
# Copy this file to .env and update the values for your environment
# DO NOT commit .env file to version control!
# ==============================================================================

# ------------------------------------------------------------------------------
# Flask Application Settings
# ------------------------------------------------------------------------------
# Generate a secure secret key: python -c "import secrets; print(secrets.token_hex(32))"
FLASK_SECRET_KEY=your-secure-secret-key-change-this-in-production

# Environment: development, production, testing
FLASK_ENV=development

# Enable debug mode (only for development!)
FLASK_DEBUG=true

# Host and port for the Flask application
APP_HOST=0.0.0.0
APP_PORT=8000

# ------------------------------------------------------------------------------
# Database Configuration
# ------------------------------------------------------------------------------
# PostgreSQL connection string
# Format: postgresql://username:password@host:port/database
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/nimbus

# ------------------------------------------------------------------------------
# Ollama Configuration
# ------------------------------------------------------------------------------
# URL to your Ollama API server
# Local: http://localhost:11434
# Remote: http://your-server-ip:11434
OLLAMA_URL=http://localhost:11434

# ------------------------------------------------------------------------------
# Document Processing
# ------------------------------------------------------------------------------
# Allowed file extensions (comma-separated)
ALLOWED_EXTENSIONS=.pdf,.md,.txt,.docx,.pptx

# Default text splitting parameters
DEFAULT_CHUNK_SIZE=1000
DEFAULT_CHUNK_OVERLAP=200

# Default embedding model to use
DEFAULT_EMBEDDING_MODEL=nomic-embed-text

# ------------------------------------------------------------------------------
# RAG Configuration
# ------------------------------------------------------------------------------
# Number of chunks to retrieve per embedding model
RAG_TOP_K_PER_MODEL=5

# Total number of chunks to include in LLM context
RAG_TOP_K_OVERALL=10

# Maximum characters per snippet sent to LLM
RAG_SNIPPET_MAX_CHARS=800

# System instruction for strict document-based answers
STRICT_DOCS_INSTRUCTION="You are given a set of retrieved document snippets which are the only allowed source of truth for this conversation. If user greets you, You can welcome him...and You MUST NOT use outside knowledge or hallucinate. Answer only from the provided documents. If the answer cannot be found in the documents, respond exactly: 'I don't know'. Be concise."

# ------------------------------------------------------------------------------
# Embedding Models Configuration
# ------------------------------------------------------------------------------
# Models to exclude from chat interface (comma-separated)
# These are embedding-only models that cannot be used for chat
EMBEDDING_MODELS=mxbai-embed-large,nomic-embed-text,all-minilm,snowflake-arctic-embed,bge-m3,bge-large,paraphrase-multilingual

# Model to Embedding Table Mapping
# Format: chat_model=table1:embedding_model1|table2:embedding_model2;chat_model2=...
# Example: Map llama3 to query both nomic and mxbai embedding tables
MODEL_EMBEDDING_TABLE_MAP=llama3:latest=document_embeddings_nomic_embed_text:nomic-embed-text|document_embeddings_mxbai_embed_large:mxbai-embed-large|document_embeddings_all_minilm:all-minilm

# ------------------------------------------------------------------------------
# Session Configuration
# ------------------------------------------------------------------------------
# Session timeout in seconds (default: 1 hour)
SESSION_TIMEOUT=3600

# Make sessions permanent (survive browser restarts)
SESSION_PERMANENT=true

# ------------------------------------------------------------------------------
# Chat Configuration
# ------------------------------------------------------------------------------
# Maximum number of chat sessions to retrieve in history
CHAT_MAX_HISTORY=50

# Timeout for Ollama chat requests (seconds)
CHAT_REQUEST_TIMEOUT=60

# Timeout for embedding generation requests (seconds)
EMBEDDING_REQUEST_TIMEOUT=20

# Timeout for model list requests (seconds)
MODELS_REQUEST_TIMEOUT=5

# ------------------------------------------------------------------------------
# Logging Configuration
# ------------------------------------------------------------------------------
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# ==============================================================================
# Production Deployment Notes
# ==============================================================================
# 
# 1. SECURITY:
#    - Generate a strong FLASK_SECRET_KEY (minimum 32 characters)
#    - Use strong database credentials
#    - Set FLASK_DEBUG=false in production
#    - Use HTTPS for all connections
#    - Change default admin password immediately
#
# 2. PERFORMANCE:
#    - Consider using a production WSGI server (gunicorn, uWSGI)
#    - Enable database connection pooling
#    - Set up reverse proxy (nginx, Apache)
#    - Consider adding Redis for caching
#
# 3. MONITORING:
#    - Set LOG_LEVEL=WARNING or ERROR in production
#    - Set up log aggregation (ELK, Splunk, etc.)
#    - Monitor database connections and query performance
#    - Track API response times
#
# ==============================================================================
